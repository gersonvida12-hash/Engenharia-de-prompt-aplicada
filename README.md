# Interface Multimodal com LLaMA Local - Desktop App para Windows 11 Pro

Uma aplica√ß√£o desktop moderna desenvolvida em Electron + React + TypeScript que permite intera√ß√£o multimodal com modelos LLaMA locais atrav√©s do Ollama.

## üìã Funcionalidades

- ‚úÖ **Interface Desktop Nativa**: Aplica√ß√£o Electron para Windows 11 Pro
- ‚úÖ **Integra√ß√£o com LLaMA Local**: Conecta com modelos LLaMA via Ollama
- ‚úÖ **Upload de Arquivos Multimodal**: Suporte para imagens, documentos, √°udio e v√≠deo
- ‚úÖ **Interface Responsiva**: Design moderno com suporte a tema escuro/claro
- ‚úÖ **Processamento em Background**: Web Workers para processamento de arquivos
- ‚úÖ **Conex√£o Configur√°vel**: URL do servidor LLaMA configur√°vel
- ‚úÖ **Sele√ß√£o de Modelos**: Interface para escolher diferentes modelos LLaMA

## üöÄ Pr√©-requisitos

### Para Windows 11 Pro:

1. **Node.js** (vers√£o 18 ou superior)
   - Baixe em: https://nodejs.org/
   
2. **Ollama** (para executar modelos LLaMA localmente)
   - Baixe em: https://ollama.ai/
   - Instale um modelo: `ollama pull llama2` ou `ollama pull llama3`

3. **Git** (opcional, para desenvolvimento)
   - Baixe em: https://git-scm.com/

## üì¶ Instala√ß√£o e Uso

### Op√ß√£o 1: Execut√°vel Pr√©-constru√≠do (Recomendado)

1. Baixe o arquivo `.exe` da se√ß√£o Releases
2. Execute o instalador seguindo as instru√ß√µes
3. Inicie o Ollama: `ollama serve`
4. Execute a aplica√ß√£o atrav√©s do √≠cone no desktop

### Op√ß√£o 2: Desenvolvimento Local

1. **Clone o reposit√≥rio:**
   ```bash
   git clone https://github.com/gersonvida12-hash/Engenharia-de-prompt-aplicada.git
   cd Engenharia-de-prompt-aplicada
   ```

2. **Instale as depend√™ncias:**
   ```bash
   npm install
   ```

3. **Execute em modo de desenvolvimento:**
   ```bash
   # Terminal 1: Inicie o Ollama
   ollama serve
   
   # Terminal 2: Execute a aplica√ß√£o
   npm run electron:dev
   ```

4. **Construir execut√°vel para Windows:**
   ```bash
   npm run dist:win
   ```
   O execut√°vel ser√° criado na pasta `release/`

## üîß Configura√ß√£o do LLaMA

### Instalando e Configurando o Ollama:

1. **Baixe e instale o Ollama:**
   - Windows: https://ollama.ai/download/windows
   
2. **Inicie o servi√ßo:**
   ```bash
   ollama serve
   ```

3. **Baixe modelos LLaMA:**
   ```bash
   # Modelos recomendados
   ollama pull llama2          # Modelo base (~3.8GB)
   ollama pull llama3          # Modelo mais recente (~4.7GB)
   ollama pull codellama       # Especializado em c√≥digo (~3.8GB)
   ollama pull mistral         # Alternativo r√°pido (~4.1GB)
   ```

4. **Verificar modelos instalados:**
   ```bash
   ollama list
   ```

### Configura√ß√£o na Aplica√ß√£o:

- **URL padr√£o**: `http://localhost:11434`
- **Porta padr√£o do Ollama**: 11434
- Teste a conex√£o usando o bot√£o "Testar Conex√£o" na interface

## üéØ Como Usar a Aplica√ß√£o

### 1. Verificar Conex√£o
- Certifique-se que o status mostra "Conectado ao LLaMA"
- Se n√£o conectar, verifique se o Ollama est√° rodando

### 2. Selecionar Modelo
- Use o dropdown "Modelo" para escolher o modelo LLaMA desejado
- Modelos dispon√≠veis aparecer√£o automaticamente

### 3. Intera√ß√£o com Texto
- Digite sua pergunta no campo de texto
- Clique em "Enviar" para obter resposta

### 4. Upload de Arquivos
- Arraste arquivos para a √°rea de upload OU
- Clique em "Procurar Arquivos"
- Tipos suportados:
  - **Imagens**: PNG, JPEG, WebP, HEIC, HEIF
  - **Documentos**: PDF, DOCX, PPTX, XLSX, RTF, TXT, CSV
  - **√Åudio**: WAV, MP3, AIFF, AAC, OGG, FLAC
  - **V√≠deo**: MP4, MPEG, MOV, AVI, WebM, WMV, 3GPP

### 5. An√°lise Multimodal
- Fa√ßa upload de arquivos + digite prompt
- O LLaMA analisar√° o conte√∫do dos arquivos junto com seu texto

## üõ† Scripts de Desenvolvimento

```bash
# Desenvolvimento
npm run dev              # Servidor de desenvolvimento (web)
npm run electron:dev     # Desenvolvimento Electron

# Build
npm run build           # Build do frontend
npm run build:electron  # Build do processo principal Electron
npm run dist           # Criar execut√°vel multiplataforma
npm run dist:win       # Criar execut√°vel apenas para Windows

# Teste
npm run electron       # Executar aplica√ß√£o Electron diretamente
```

## üìÅ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ electron/           # C√≥digo Electron (processo principal)
‚îÇ   ‚îú‚îÄ‚îÄ main.ts        # Processo principal
‚îÇ   ‚îú‚îÄ‚îÄ preload.ts     # Script de preload
‚îÇ   ‚îî‚îÄ‚îÄ tsconfig.json  # Config TypeScript para Electron
‚îú‚îÄ‚îÄ src/               # C√≥digo React
‚îÇ   ‚îú‚îÄ‚îÄ components/    # Componentes React
‚îÇ   ‚îú‚îÄ‚îÄ services/      # Servi√ßos (LLaMA API)
‚îÇ   ‚îú‚îÄ‚îÄ styles/        # Estilos CSS
‚îÇ   ‚îî‚îÄ‚îÄ types/         # Defini√ß√µes TypeScript
‚îú‚îÄ‚îÄ public/            # Arquivos est√°ticos
‚îú‚îÄ‚îÄ dist/              # Build do frontend
‚îú‚îÄ‚îÄ dist-electron/     # Build do Electron
‚îî‚îÄ‚îÄ release/           # Execut√°veis finais
```

## üîß Personaliza√ß√£o

### Configurar Modelos Customizados:
1. Adicione modelos no Ollama: `ollama pull [modelo]`
2. Selecione na interface ou configure via c√≥digo

### Configurar URL Customizada:
- Altere a URL na interface para usar servidor remoto
- Exemplo: `http://192.168.1.100:11434`

### Temas e Estilos:
- Edite `src/styles/global.css`
- Suporte autom√°tico a tema escuro/claro do sistema

## üö® Solu√ß√£o de Problemas

### LLaMA n√£o conecta:
1. Verifique se Ollama est√° rodando: `ollama serve`
2. Teste no navegador: http://localhost:11434/api/tags
3. Verifique firewall/antiv√≠rus

### Arquivo n√£o √© processado:
1. Verifique se o tipo de arquivo √© suportado
2. Verifique se o arquivo n√£o excede 20MB
3. Tente com arquivo menor primeiro

### Performance lenta:
1. Use modelos menores (llama2 vs llama3)
2. Verifique recursos do sistema (RAM, CPU)
3. Configure GPU se dispon√≠vel

### Execut√°vel n√£o inicia:
1. Execute como administrador
2. Verifique se todas as depend√™ncias est√£o instaladas
3. Verifique logs em `%APPDATA%/interface-llama-desktop`

## üìã Requisitos do Sistema

### M√≠nimos:
- **OS**: Windows 11 Pro (64-bit)
- **RAM**: 8GB (recomendado 16GB+)
- **Armazenamento**: 10GB livres
- **Processador**: Intel i5/AMD Ryzen 5 ou superior

### Para modelos LLaMA maiores:
- **RAM**: 16GB+ (32GB recomendado)
- **GPU**: NVIDIA RTX (opcional, para acelera√ß√£o)

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit: `git commit -m 'Add nova funcionalidade'`
4. Push: `git push origin feature/nova-funcionalidade`
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## üÜò Suporte

- **Issues**: https://github.com/gersonvida12-hash/Engenharia-de-prompt-aplicada/issues
- **Documenta√ß√£o Ollama**: https://ollama.ai/docs
- **Documenta√ß√£o Electron**: https://electronjs.org/docs

---

‚ö° **Transformado com sucesso de uma interface web Gemini para um aplicativo desktop Windows 11 Pro com LLaMA local!**
